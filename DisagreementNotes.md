---
# A
---

#### Akerlof, G. A., & Michaillat, P. (2018). Persistence of false paradigms in low-power sciences. _Proceedings of the National Academy of Sciences_, _115_(52), 13228–13233. [https://doi.org/10.1073/pnas.1816454115](https://doi.org/10.1073/pnas.1816454115)

#model #theory #kuhnian #progress 

The authors present a mathematical model of paradigm shifts, backed up by several different case studies. 

The basic idea is this: the *tenure* selection process, combined with homophily, can lead to situations where the worst of two paradigms persists. In evaluations, academics will to favor those who belong to a similar paradigm. Therefore, if one paradigm is mostly dominant, and a candidate from a secondary paradigm goes on the market, then they are not only very likely to be evaluated by the opposing paradigm, but also evaluated more poorly. In cases where this homophily preference is high, and where the power of the evidence is relatively low, bad paradigms can persist, or better ones will never have the chance to get off the ground


---
# B
---

#### Beebe, J. R., Baghramian, M., Drury, L., & Dellsén, F. (2019). Divergent Perspectives on Expert Disagreement: Preliminary Evidence from Climate Science, Climate Policy, Astrophysics, and Public Opinion. _Environmental Communication_, _13_(1), 35–50. [https://doi.org/10.1080/17524032.2018.1504099](https://doi.org/10.1080/17524032.2018.1504099)

#case #survey #measurement #climate #astrophysics

The authors survey experts and non-experts in both Climate Science and Astrophysics in order to assess their relative beliefs about disagreement in their fields. 

For climate scientists, they find that, as compared to non-experts, **climate experts** beleive that:
1. There is less disagreement within climate science about climate change
2. More of the disagreement that does exist concerns public policy questions rather than the science itself,
3. Methodological factors play less of a role in generating existing disagreement among experts about climate science
4. Fewer personal and institutional biases influence the nature and direction of climate science research, 
5. There is more agreement among scientists about which methods or theoretical perspectives should be used to examine and explain the relevant phenomena, 
6. Disagreements about climate change should not lead people to conclude that the scientific methods being employed today are unreliable or incapable of revealing the truth, and 
7. Climate science is more settled than ideological pundits would have us believe and settled enough to base public policy on it.

In contrast, experts in Astrophysics were more willing to acknowledge disagreements in their field, less quick to dismiss lay people, more open to the possibility that data have more than one interpretation.



#### Bordignon, F. (2022). Critical citations in knowledge construction and citation analysis: From paradox to definition. _Scientometrics_. [https://doi.org/10.1007/s11192-021-04226-0](https://doi.org/10.1007/s11192-021-04226-0)

#Disagreement #critical #criticism #meta #litreview


Here, the author examines the notion of "critical citations", that is citations that aim to criticize a past work. Specifically, they create a synthesis definition of critical citations by drawing on definitions and labels from over 50 studies on citation classification. In doing so, they create three general categories of critical citation:

**To criticize:** To directly point out a weakness or fault in the cited work. Will generally *only* mention the targeted work


**To compare:** Compare two studies, without explicitly expressing a claim over whether or not one is better than the other. Usually involves two studies (often the author's)

**To question:** To express doubt or concern about a past work.


An additional piece from the paper that I found interesting: citations are suggested to be, by default, *positive* in polarity because they it takes a commitment on the part of the author. Negative citations are an even larger commitment.

---
# C
---

#### Choi, Y., Jung, Y., & Myaeng, S.-H. (2010). Identifying Controversial Issues and Their Sub-topics in News Articles. In H. Chen, M. Chau, S. Li, S. Urs, S. Srinivasa, & G. A. Wang (Eds.), _Intelligence and Security Informatics_ (Vol. 6122, pp. 140–153). Springer Berlin Heidelberg. [https://doi.org/10.1007/978-3-642-13601-6_16](https://doi.org/10.1007/978-3-642-13601-6_16)

#SocialMedia #detection #sentiment #controversy 

The authors present a method of automatically identifying controversial topics from newspaper articles. Here it is based on sentiment, roughly measuring the amount of positive and negative sentiment associated with each topic. They generate potential controversial topics through some kind of known-entity generation algorithm that I don't fully understand, but which seems interesting. 

This is primarily interesting for the use of sentiment-like features for identifying controversy, as we could develop something similar. 


#### Christensen, D. (2007). Epistemology of Disagreement: The Good News. _The Philosophical Review_, _116_(2), 187–217.

#theory #philosophy #epistemology #disagreement 

This is a long, and honestly meandering paper, addressing the question of whether epistemic peers should adjust their beliefs in response to disagreements. 

Here, the author comes down on the position that, yes, it is usually rational, in many circumstances, to adjust ones confidence in their own belief in response to their peers holding different ones. 


#### Cleland, C. E. (2002). Methodological and Epistemic Differences between Historical Science and Experimental Science*. _Philosophy of Science_, _69_(3), 447–451. [https://doi.org/10.1086/342455](https://doi.org/10.1086/342455)
#disciplines #epistemology #historical #experimental


Here, Cleland examines the epistemic differences between so-called *experimental* and *historical* sciences. 

Experimental sciences are those that can conduct controlled experiments in order to support knowledge claims. Think of the Large Hadron Collider, or the Chemistry Labs. Hisotrical sciences, in contrast, attempt to reconstruct past events and cannot conduct such experiments. Think of trying to understanding tectonic plates, or what killed the dinosaurs. 

While there has, at times, been tension between these two sides (with experimental scientists disregarding historical research as "science" altogether), Cleland argues that they are actually more similar then they appear. 

Namely, both experimentalists and historical scientists attempt to infer causes from effects, their ways of doing so rely on different kinds of evidence and methodological approaches. Namely, experimental sciences are *under-determined*, namely that the evidence is not enough to distinguish between theories. Historical sciences, in contrast, are *over-determined*, meaning that there is a great deal of evidence, many combinations of which can be used to support different theories. These key distinctions drive the epistemic differences in these fields. 


### Coletto, M., Garimella, K., Gionis, A., & Lucchese, C. (2017). Automatic controversy detection in social media: A content-independent motif-based approach. _Online Social Networks and Media_, _3–4_, 22–31. [https://doi.org/10.1016/j.osnem.2017.10.001](https://doi.org/10.1016/j.osnem.2017.10.001)

#method #network #controversy #SocialMedia 

Here the authors present a novel and effective way of automatically detecting controversies on social media, namely a network-based approach that searches for *graph motifs*. The key idea is that controversial discussions on social media have different reply structures than non-controversial ones, namely they are deeper, with many instances of individuals responding to one another, who are not themselves "following" or "friends" with each other. The authors search for these discussion tree structures by searching for certain "graph motifs", which are local-level patterns of interaction, defined by their replies and "friendship" linkages. These motifs are shown to be better predictors of controversy than other approaches, able to even identify controversial discussions within sub-topics. Moreover, they are not reliant on messy NLP or language-specific approaches. 


#### Collins, R. (1994). Why the social sciences won’t become high-consensus, rapid-discovery science. _Sociological Forum_, _9_(2), 155–177. [https://doi.org/10.1007/BF01476360](https://doi.org/10.1007/BF01476360)

#theory #disciplines #SocialScience 

Here, the author attempts to understand what makes the natural sciences different form the social sciences. Namely, they state that the natural sciences are *rapid discovery* disciplines, characterized by their rapidly-moving research frontier and high levels of consensus. The social scineces, by contrast, bear more in resemblance to philosophy, or natural philosophy of older times, in that they are divided into competing schools of thought, constantly re-tread old disagreements, and do not make rapid progress. So whereas the social sciences are characterized by their disagreements, natural sciences are characterized by their consensus. 

In exploring the characteristics of each discipline, the author find that empiricism, quantification and measurement, and experimentation are not alone sufficient to explain the differences between these two kinds of fields. Rather, the author argues that *geneaologies of research technology* are the key difference. 

By *research technology*, the author means instruments that produce interesting effects, such as Boyle's air pump or other interesting experimental devices. These machines provide test beds for researchers to produce interesting effects. Moreover, they can be modified to yield yet more interesting effects, allowing for other scientists to identify novel insights to boost their career. These research technologies allow scientists to continuously produce interesting and novel effects, and establish clear consensus. 

Social sciences do not have any clear candidates for such research technologies that produce novel effects. And so the field continuously fights over fundamental disagreements and differences in schools of thought. Without such technologies, the author argues that the social sciences will **never** become a rapid discovery discipline


#### Cook, J., Nuccitelli, D., Green, S. A., Richardson, M., Winkler, B., Painting, R., Way, R., Jacobs, P., & Skuce, A. (2013). Quantifying the consensus on anthropogenic global warming in the scientific literature. _Environmental Research Letters_, _8_(2), 024024. [https://doi.org/10.1088/1748-9326/8/2/024024](https://doi.org/10.1088/1748-9326/8/2/024024)

#climate #consensus #measure 

The authors examine over 11,000 thousand papers that mention the word "global warming" or "climate change" in their title, and manually classify them as to whether they explicitly endorse human-caused climate change, reject it, or take no position. 

They find that the amount of "no position" has increased over time, roughly consistent with a view of consensus where scientific attention has moved to other subjects. 


#### Cruz, H. D., & Smedt, J. D. (2013). The value of epistemic disagreement in scientific practice. The case of Homo floresiensis. _Studies in History and Philosophy of Science Part A_, _44_(2), 169–177. [https://doi.org/10.1016/j.shpsa.2013.02.002](https://doi.org/10.1016/j.shpsa.2013.02.002)

#disagreement #casestudy #history #paleontology


This study considers the potential value of disagreement between [[epistemic peers]] in a scientific community, focusing on the particular case of *homo floresiensis*, the pygmy hominid whose bones were found in an island in Indonesia. The disagreement centers on the question of whether these bones are of a species distinct from *Homo Sapien*, or whether they were instead of modern *Homo Sapien* having some clinical pathology. Both sides have roughly similar expertise and access to the same evidence, yet they also hold different interpretations of that evidence that lead them to different conclusions, what is called [[deep epistemic disagreement]].

Using *Homo Floresiensis* as a case study, the authors argue that disagreements benefit the field and science as a whole, through three distinct mechanisms:

**The generation of new evidence**: THe presence of a disagreement in the field motivates scientists to pursue new evidence for their position, in the hopes of identifying a "smoking gun" or some other trace that supports their side, or discredits the other. Moreover, disagreement encourage the search for *theory-independent* evidence, that is evidence which does not rely on the theoretical assumptions of one or the other side, and thus is more difficult to be dismissed. 


**Re-evaluation of existing evidence and assumptions**
Disagreements also motivate scientists to question their own assumptions and look closer at their evidence, something which they might not otherwise have done in the absence of conflict. 

**An antidote for confirmation bias** 
Confirmation and discomfirmation biases are as common to scientists as everyone else. However, by engaging opposing views, scientists are forced to re-examine their assumptions and understand competing arguments, as well as to pay attention to anomalous data, which could easily be dismissed without a controversy, 


Apart from these arguments, the paper also does a good job of summarizing some of the key ideas and concepts surrounding the conversation of disagreement in the field of philosohy of science and epistemology. 


---
# D 
---

#### De Cruz, H., & De Smedt, J. (2012). Evolved cognitive biases and the epistemic status of scientific beliefs. _Philosophical Studies_, _157_(3), 411–429. [https://doi.org/10.1007/s11098-010-9661-6](https://doi.org/10.1007/s11098-010-9661-6)

#Bias #cognition #disagreement #progress #theory 

The authors investigate the role of evolved cognitive biases and how these affect what we think about science. Namely, the consider two distinct viewpoints: Evolutionary Arguments for science as a justified belief, and Evolutionary Debunking Arguments for why we might *not* be justified for having these beliefs. 

**Evolutionary argument:**

1. Animals that successfully interact with the world would have a higher chance of survival
2. Beliefs that accurately track the world are typically better than false beliefs
3. Therefore, natural selection will favor animals with accurate understnadings of the world
4. Common sense beliefs have direct bearing on human fitness
5. Therefore, common sense beliefs will tend to be correct

So intuitions, upon which science is based, should be likely to be correct given evolutionary pressures


**Evolutionary debunking argument:** 

Alternatively, we can consider how scientific beliefs might *not* be justified given evolution. 

- Many false beliefs do not compromise fitness
- Natural selection may in fact favor incorrect cognitive faculties in certain circumstances. I.e., promoting cautionary perceptions, such as the belief of a predator or enemy in the darkness, even when it would not be justified to beleive this
- Because time and cognitive processes are limited, fact yet imprecise heuristics could be favored in natural selection over slower and more careful deliberation
- Some false beliefs might actually be beneficial towards survival

Given these factors, its clear to see how, given the evolutionary argument, any belief should not be justified, even beliefs grounded in science. 

**Cultural transmission of beliefs:**
The authors go on to argue that even though we may not be individually justified in our beliefs, that a model of cultural transmission instead argues that collectively, we can still expect scientific progress. Namely, this progress will occur so long as there is a large enough pool of scientists positing a diverse enough range of beliefs. Through their disagreement and counter-argument, we should still expect progress and nearness to the truth, even though individually they are likely to be incorrect. 


#### de Zarate, J. M. O., Di Giovanni, M., Feuerstein, E. Z., & Brambilla, M. (2020). Measuring Controversy in Social Networks Through NLP. In C. Boucher & S. V. Thankachan (Eds.), _String Processing and Information Retrieval_ (pp. 194–209). Springer International Publishing. [https://doi.org/10.1007/978-3-030-59212-7_14](https://doi.org/10.1007/978-3-030-59212-7_14)

#methods #embedding  #SocialMedia #controversy 

The authors develop a content-based approach for measuring the degree of controversy surrounding a topic on social media. It has a 4-step procedure:

1. They first build a conversation graph between users for tweets in a given topic
2. Then, they identify communities within the resulting grph
3. The most central users in the conversation graph are extracted, and embeddings created of their tweets with BERT and FastText. 
4. A "controversy score" is calculated from the embeddings, roughly capturing the degree of separation between clusters. A controversial topic, it is argued, will have highly distinct language patterns, reflected in the embedding space


#### Dieckmann, N. F., Johnson, B. B., Gregory, R., Mayorga, M., Han, P. K. J., & Slovic, P. (2017). Public perceptions of expert disagreement: Bias and incompetence or a complex and random world? _Public Understanding of Science_, _26_(3), 325–338. [https://doi.org/10.1177/0963662515603271](https://doi.org/10.1177/0963662515603271)

#PublicComm #Controveresy #Consensus #survey 

The authors survey participants about their perceptions of why scientific experts disagree. First, they develop their own scale to be used in the survey, identifying three key contexts for how lay persons (the participants) think about the controversy, namely:
1. One group is simply more competent than the other
2. One or both groups are influenced by their bias and self-interest
3. The topic is too complex and uncertain for the scientists to, at present, come to agreement

Interestingly, in this study, the authors find that American participants often conflate process/competence (1) and self interest (2), which may indicate something unique about U.S. perception of science. 

In applying their scale, the authors also investigate what factors drive the perceptions of scientific dispute. They find:
- For some topics, knowledge of science drives reason (2)
- For others, increased knowledge leads to less adherence to complexity/uncertainty as a reason (3)
- Belief in scientific positivism didn't have a strong effect
- Political ideology also matters to the values/interest reason (2), particularly conservative ideology
- Conspiratorial ideation is also related to perceptions, particularly the values/interest (2) one


#### Duede, E., & Evans, J. (2021). The Social Abduction of Science. _ArXiv:2111.13251 [Physics]_. [http://arxiv.org/abs/2111.13251](http://arxiv.org/abs/2111.13251)

#Theory #Science #Abduction #Induction #Progress #Deduction

This is an interesting theoretical paper, that while long and perhaps a little wordy, has a clear and powerful point. Namely, they propose that science advances not through *induction* (data to theory) or *deduction* (theory to data), but rather through *Abduction*. 

Here, abduction is defined as the process of forming an explanatory hypothesis, of studying facts and devising theory to explain them. Basically, its a matter of coming up with the best explanation for the information at hand. 

However, the authors also argue that abduction is not alone able to explain  breakthrough science, which seems improbable in the real world. This is because:
1) Scientists must first be able to identify an anomaly—a surprising result. This requires that they have a firm understanding of their fields and what they expect
2) They must then be able to reason broadly, identifying potential explanations to make the "surprising" into the "unsurprising". However, the very same disciplinary mindset that allows them to identify the anomaly also limits their ways of thinking, leaving them ignorant of possible explanations

The solution to this dilemma, the authors argue, is *social*. Namely, the person who identifies the anomaly and who offers the explanation will most often be different people. Namely, the explanation is like to come from disciplinary *outsiders*, who bring with them a set of potential explanations and ideas that insiders would never generate themselves. 


---
# E
---

#### Ellemers, N., Fiske, S. T., Abele, A. E., Koch, A., & Yzerbyt, V. (2020). Adversarial alignment enables competing models to engage in cooperative theory building toward cumulative science. _Proceedings of the National Academy of Sciences_, _117_(14), 7561–7567. [https://doi.org/10.1073/pnas.1906720117](https://doi.org/10.1073/pnas.1906720117)

#Disagreement #Cooperation #Psychology #theory 

The authors report their experiences in a workshop where they pursued different negotiation strategies to try and resolve adversarial theoretical views between psychologists. They argue that by explitiely acknowledging the legitimacy of each other's positions and cooperating to build theories together, that they increased trust between the opposing sides and made some progress. 


#### Evans, J. H. (2007). Consensus and knowledge production in an academic field. _Poetics_, _35_(1), 1–21. [https://doi.org/10.1016/j.poetic.2007.01.001](https://doi.org/10.1016/j.poetic.2007.01.001)

#discursive #linguistic #consensus #kuhnian #production

The author explores how *[[Discursive consensus]]*, that is the degree to which authors in a field adhere to the samse linguistic symbols, relates to the field's level of production. Four hyopthesis are considered, either increased consensus leads to 1) increased production in the field, or 2) decreased production, or the effect runs the other way, such that increased production 3) leads to increased consensus or 4) to decreased consensus. 

The author focuses on the field of Bioethics, which functions largely as a discipline in the humanities. Examining 50 "sub-debates" identified using subject categories used to classify papers in their purpose-built bibliographic database, the author observes the first hypothesis to be true, namely that increased consensus leads to increased production in the field. 


---
# G
---

#### Garimella, K., Morales, G. D. F., Gionis, A., & Mathioudakis, M. (2018). Quantifying Controversy on Social Media. _ACM Transactions on Social Computing_, _1_(1), 3:1-3:27. [https://doi.org/10.1145/3140565](https://doi.org/10.1145/3140565)

#SocialMedia #networks #controversy #EchoChambers

This is a really great papers in which the authors give a broad overview of quantifying the level of controversy surrounding a topic on social media, in this case using Twitter and defining a topic as a hashtag, such as #Resist or #DCProtests. Their approach to defining controversy is really similar to Shwed & Bearman's paper on consensus in science, namely that a controversial topic should develop a "clustered" network structure. Here, users make up the nodes of the network, and edges are defined based on their connections via a) direct endorsement (follow, retweet), sharing of symbols (same hashtags, linked urls), and similar content (similar words, e.g., bag of words). 

The authors select a series of topics known to be either controversial or non-controversial, construct a conversation graph using tweets involved in each topic, partition them into "clusters" or "sides", and determine the ability of their various network measures to distinguish between the clusters. 

The best performing measure of "consensus" of the network is a random walk based approach, the intuition for which is that in a controversial network, a random walker starting in one cluster should be likely to end in the same cluster. Additionally, they observe that sentiment analysis of text holds some promise–controversial topics are debated more intensely with more charged language. 


#### Geras, A., Siudem, G., & Gagolewski, M. (2020). Should we introduce a dislike button for academic articles? _Journal of the Association for Information Science and Technology_, _71_(2), 221–229. [https://doi.org/10.1002/asi.24231](https://doi.org/10.1002/asi.24231)

#dislike #disagreement #stackexchange #citations 

Here the authors consider the case of negative citations. Specifically, they model signed citations using "likes" and "dislikes" on StackExchange, to get a feel for their dynamics. What they find is that, surprisingly, there is actually a slight *positive* correlation between "likes" (position/neutral citations) and "dislikes" (negative citations). Those questions on StackExchange that get the highest absolute value of likes **also** receive the highest absolute value of dislikes. 

This results has a few implications. For one, a "dislike" button in science probably wouldn't be very informative, since it would just be highest for the most highly-cited articles. Second, it seems that the worst fate in StackExchange, and likely in science, is not to receive negative citations but to be ignored entirely—it is all about attention. Therefore, if anything, negative citations are just an indicator of *where attention is being directed*


---
# H
---

#### Hargens, L. L., & Hagstrom, W. O. (1982). Scientific Consensus and Academic Status Attainment Patterns. _Sociology of Education_, _55_(4), 183–196. [https://doi.org/10.2307/2112671](https://doi.org/10.2307/2112671)

#career-trajectory #careers #success #consensus 

The authors test the idea that the level of consensus in a field will have an impact on the academic attainment patterns of its members. Namely, in a field with high consensus, talent is expected to be identified early, resulting in talented individuals quickly getting into top programs and finding success. In contrast, in lower-consensus fields, success should be more random, and talented individuals are less likely to be identified early.

Basically, in high-consensus fields, those who have the best performance = those who went to the best schools. In low-consensus fields, institutional prestige should be less-strongly related to performance. 

The authors find evidence of their hypothesis, though only with a few hundred careers across the fields of Physics, Chemistry, Biology, Mathematics, and Political Science.


#### Hargens, L. L. (1988). Scholarly Consensus and Journal Rejection Rates. _American Sociological Review_, _53_(1), 139–151.

#RejectionRates #journals #consensus #disciplines 

Here, the author argues that journal rejection rates are potential signals of the level of consensus within an academic field. Rejection rates differ between fields, and the authors show this likely isn't the result of differences in space in their respective journals. Rather, they attribute it to structural differences between the fields. Namely, higher journal rejection rates indicate that 

Namely, when journal rejection rates are higher, it implies that reviewers do not share the same priorities, conceptual frameworks, theories, or methods as the authors they are reviewing, indicating a lower level of consensus in their field. 


#### Hilgartner, S., & Bosk, C. L. (1988). The Rise and Fall of Social Problems: A Public Arenas Model. _American Journal of Sociology_, _94_(1), 53–78.

#theory #model #debate #controversy #public

This paper outlines the "public arenas model" of social problems. A *social problem* is generally just something that a community or the public broadly agrees is a problem. The framework roughly goes like this:

Social problems are dedicated in *public arenas*. Public arenas are institutions that manage public discourse for some social context, such as newspapers, TV programs, social media platforms, etc.; each public arena has its own *carrying capacity*: a limited amount of space (minutes, pages, etc.) that can be devoted to each problem, and the intensity of a problem is how much space is devoted to it. Individual community members, too, have limited attention to spare for social problems. There is a broad ecosystem of social problems that compete for this limited space, with their fitness judged by their level of drama, emotional intensity, and novelty. Cultural & political norms also come into play to determine which problems are considered more worthy of space. Singular operatives too, members of the community or other organizations, can promote and push social problems, drawing more attention to them. However, when problems are around for too long they become boring and are valued less. This grand competition is the foundation of the rise and fall of social problems. 

Its an interesting and well laid-out paper, and should be read in more depth later. 


---
# L
---

Lewandowsky, S., Gignac, G. E., & Vaughan, S. (2013). The pivotal role of perceived scientific consensus in acceptance of science. _Nature Climate Change_, _3_(4), 399–404. [https://doi.org/10.1038/nclimate1720](https://doi.org/10.1038/nclimate1720)

#SciComm #PublicComm  #Consensus #climate 

The authors examine how perceived scientific consensus shapes respondents (random citizens on the street) views of climate change. They find that in every case considered (climate change, smoking & lung cancer, etc.), the respondents under-estimated the degree of scientific consensus. In the case of Climate Change, the average perceived consensus was 70%, whereas in reality it is upwards of 97 (at the time of the paper's publication). 

Through structural equation model, the authors observe that an underlying attitude pervades the responses to the perceived consensus across science, roughly corresponding to a general distrust of science. 

The authors then gave the respondents information about the actual consensus on this topic, which caused their opinions and estimations to shift.

So, perceived consensus is the strongest predictor of belief in these pieces of information, and by expressing this consensus, people's belief & trust in the science can shift too. 



---
# M
---

#### Marres, N., & Moats, D. (2015). Mapping Controversies with Social Media: The Case for Symmetry. _Social Media + Society_, _1_(2), 2056305115604176. [https://doi.org/10.1177/2056305115604176](https://doi.org/10.1177/2056305115604176)

#SocialMedia #controversy #qualitative #STS

The authors examine the theoretical concerns around the study of controversies on social media. The paper itself isn't super helpful for my research, but they do a good job summarizing the origins and early work in controversy analysis, and its particular application to social media. 


---
# P
---

#### Petersen, A. M., Vincent, E. M., & Westerling, A. L. (2019). Discrepancy in scientific authority and media visibility of climate change scientists and contrarians. _Nature Communications_, _10_(1), 3502. [https://doi.org/10.1038/s41467-019-09959-4](https://doi.org/10.1038/s41467-019-09959-4)

#Climate #Contrarian #controversy #media #news 

Here, the authors examine the difference in representation between Climate Change Scientists (i.e., prominent scientists who affirm anthropogenic climate change), and Climate Change Contrarians (those who take explicit contrarian views to the scientific consensus). The authors draw on a dataset of over 300 prolific climate change contrarians, sourced from a website called "DeSmog", and match them to a similarly-sized set of the most highly-cited climate scientists. Then, pulling news stories mentioning these individuals, they observe that contrarians are over-represented in coverage, an effect largely driven by alternative non-mainstream news sources.


---
# R
---

#### Radicchi, F. (2012). In science “there is no bad publicity”: Papers criticized in comments have high scientific impact. _Scientific Reports_, _2_(1), 815. [https://doi.org/10.1038/srep00815](https://doi.org/10.1038/srep00815)

#commenting #disagreement #success #paper

The author examines the impact that "comments" have on papers. A comment is most often published as an explicit criticism of a published paper in the journal, and so represents a formal disagreement. What the author finds is that:

- Papers that receive comments tend to be cited more highly
- Papers that receive comments are typically the most cited papers in their journal
- Comments are published quickly (~13.5 months) after the paper's publication
- Comments are less common in recent years (in the 2000s)

---
# S
---

#### Small, H. (2018). Characterizing highly cited method and non-method papers using citation contexts: The role of uncertainty. _Journal of Informetrics_, _12_(2), 461–480. [https://doi.org/10.1016/j.joi.2018.03.007](https://doi.org/10.1016/j.joi.2018.03.007)

#uncertainty #hedging 

Methods papers tend to dominate on lists of the most cited papers within a field. For example, a paper outlining a way of examining proteins is much more highly cited than Watson & Crick's paper on DNA. Here, Small examines what the citation contexts referencing these highly-cited papers can tell us about their roles in science. Specifically, Small examines how *hedging* terms, which are expressive of uncertainty, varies between different types of papers. 

What is found is that hedging terms are found to characterize contexts to non-methods papers, but not methods papers. That is, methods papers sit on a firmer epistemological footing than non-methods papers. 


---
# V
---

#### van der Bles, A. M., van der Linden, S., Freeman, A. L. J., Mitchell, J., Galvao, A. B., Zaval, L., & Spiegelhalter, D. J. (2019). Communicating uncertainty about facts, numbers and science. _Royal Society Open Science_, _6_(5), 181870. [https://doi.org/10.1098/rsos.181870](https://doi.org/10.1098/rsos.181870)


#Uncertainty #Theory #Framework #PublicComm #SciComm 

Here, the authors lay out a framework for conceiving of uncertainty—particularly epistemic uncertainty—and of its communication. Their framework is based heavily on the [Laswell model of communication](https://en.wikipedia.org/wiki/Lasswell%27s_model_of_communication), which they customize to the case of epistemic uncertainty, shown in this figure

![[Pasted image 20220302140250.png]]

Namely, this pictures the relevant features of communication *from who*, *what*, *in what form*, *to whom*, and *to what effect*.

They distinguish between three [[objects of uncertainty]]: *facts*, *numbers*, and *scientific hypotheses*; as well as two [[levels of uncertainty]]:  *direct* and *indirect*. Beyond these, the authors also outline a list of 9 alternative [[expressions of uncertainty]], ranging from communicating the whole explicit probability distribution towards total denial of uncertainty. 



---
# W
---

#### Weinberger, N., & Bradley, S. (2020). Making sense of non-factual disagreement in science. _Studies in History and Philosophy of Science Part A_, _83_, 36–43. [https://doi.org/10.1016/j.shpsa.2020.01.004](https://doi.org/10.1016/j.shpsa.2020.01.004)

#theory #disagreement #casestudy 

This is a super interesting paper about *non-factual* disagreement in the sciences. Typically, the philosophy of science has considered only disagreements in which the peers more-or-less understand what it is they are disagreeing about, and where the dispute reflects some grand philosophical debate. However, here, the authors argue that instead, many disputes in the sciences are in fact over non-factual things, where the participants don't really know what it is they are disagreeing about. 

> " We claim that disagreement in the sciences – particularly in interdisciplinary contexts – often has neither of these features. That is, scientists can in fact disagree without really being aware of the disagreement, and even when it becomes apparent that there is a disagreement, it isn’t always clear (to those disagreeing) what is actually being disagreed about."

Rather, disagreements might results from the "background methodological stances form those disagreeing", which are more often to arise in interdisciplinary research contexts. This is shown across three examples:
1. Within epidemiology, there is debate over the notion of *consistency* in defining models, roughly referring to the expectations that treatments have the same outcomes across individuals. However, consistency requires specifying, with precision, what the treatments are (motrin & tylenol are both "painkillers", but likely to have similar outcomes, need to be exact) and understanding confounding factors. However, theoreticians & experimentalists come to different viewpoints on consistency. Theoreticians argue that consistency must be a base model assumption, such that all confounding effects are appropriate modelled at the outset. Experimentalists, on the other hand, start with what is practical to measure and test, often excluding or delaying theoretical detail. This leads to conflict between the two camps, not understanding why the other doesn't use the same methodological approach. 
2. In econophysics, a similar methodological divide plays out between traditional Economists & Physicists who want to apply their methods to study the economy. Physicists modeled cash transfer in the economy based on simple mechanistic models of diffusion, yet economists hate this, as models exclude the rich detail of the economy, such as growth and credit. Both camps can effectively model the economy in a way that matches real-world data, but they have fundamental disagreements about the proper way to do so. Its a disagreement over method, not facts. 
3. In stem cell biology, dynamical systems theorists have attempted to model cell dynamics with success, however are more-or-less ignored by traditional experimental biologists. "The inability of one research community to find another research community’s work useful would seem to be sufficient for them to ignore it. Unless the DST modelers can explain to the experimentalists how they would benefit from the models, they will continue to be neglected, even if the explanatory paradigms of the two disciplines were similar"


This was a super interesting paper, I really liked it. I have recorded a series of useful quotes below,

> "We suspect that this debate between theoreticians and experimentalists plays out across a wide range of sciences."
> "One difficulty in finding the source of scientific disagreements is that the written record of disputes across articles may omit sources of disagreement that only emerge in the face-to-face interactions among the sciences, so ethnographic work is invaluable."
> "Broadly, the disputes considered result from differences in background methodological assumptions. Resolving these disputes is not merely a matter of one party convincing the other of the truth of a particular set of propositions, but rather of coming to a consensus about which modeling frameworks are best suited for making progress in a particular domain"
> "In each of the cases we have discussed, scientists from different communities are dis- agreeing with each other, and the disagreements do not, at bottom, concern first-order scientific facts. These disagreements often result from broader methodological or founda- tional differences between the communities. Because these methodological principles are often tacit, unspoken aspects of the scientists’ training, it is difficult for the practitioners to recognize the source of their disagreement with their colleagues"


#### Wu, C., Fuller, S., Shi, Z., & Wilkes, R. (2020). The gender gap in commenting: Women are less likely than men to comment on (men’s) published research. _PLoS ONE_, _15_(4), e0230043. [https://doi.org/10.1371/journal.pone.0230043](https://doi.org/10.1371/journal.pone.0230043)

#gender #commenting 

Comments are important because the represent formal disagreements, and might even promote success or indicate impact (see [[radicchi_2012_comment|(see Radicchi, 2012)]].

Here, the authors show that there is a gender gap in commenting on articles in PNAS, namely that women are less likely to publish comments overall, and also less likely to publish comments engaging with men's research. The authors argue that women's lower rates of commenting mean that they participate less in high-impact scientific debate.


---
# Y
---

#### Yu, B., Li, Y., & Wang, J. (2019). Detecting Causal Language Use in Science Findings. _EMNLP_. [https://doi.org/10.18653/v1/D19-1473](https://doi.org/10.18653/v1/D19-1473)

#ML #Measurement #Causality #Linguistic #Language 

The authors develop a classifier for identifying *causal language* within the academic literature. Their main model was based on BERT (though a few models were used) and their training data included over 3,000 hand-coded sentences pulled from the structured abstracts across 5 common health topics. The authors then use the best-performing model to study the incidence of causal language across studies explicitly labeled as observational. They find that, among these observational studies, that:

1. 21.7% of studies use **only** direct causal language in their conclusions, and 32% use some
2. That the usage of causal language differs by the country of the authors, with the highest proportion of causal language in German papers, and the lowest in U.S. papers


---
# Z
---

#### Zou, H. (Joanna), & Hyland, K. (2020). Managing evaluation: Criticism in two academic review genres. _English for Specific Purposes_, _60_, 98–112. [https://doi.org/10.1016/j.esp.2020.03.004](https://doi.org/10.1016/j.esp.2020.03.004)

#Blogs #BookReviews #Disagreement #criticism 

The authors exaine differences in criticism in two different genres of academic review: Book Reviews (which are intentionally meant to criticize), and comments on academic blog posts (which usually also serve to criticize). Specifically, they conduct a content analysis of 36 book reviews from sociology journals and 270 blog comments from *The Conversation* website. 

Their findings are mostly summarized in this quote:

> "Most importantly, we show that while blog commenters focus on similar aspects of the source text, they were more critical of everything in it, and that they often turn to criticise other commenters, the text author and general public. We also found that blog commenters mitigated these criticisms less than book reviewers and that they tended to use more personal devices to do so."


Another important notion not introduced here, but used in this study, is that of **"mitigation"**. Mitigation is the employment of strategies to save face during a criticism, such as using hedging to soften the criticism. 


