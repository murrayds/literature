## Raw Data is an Oxymoron

**Keywords**: Data Science; Objectivity; Data Cleaning

**Authors**: Edited by Lisa Gitelman

**Date of Publication**: 2013

**Reference**: Jackson, V., Rosenberg, D., Williams, T. D., Brine, K. R., Poovey, M., Stanley, M., … Bowker, G. C. (2013). “Raw Data” Is an Oxymoron (L. Gitelman, Ed.). Cambridge, Massachusetts ; London, England: The MIT Press.




#### Key Concepts
----

N/A

#### Questions
----

***What is the main argument of the text?***

The purpose of this book is to demonstrate, through a variety of historical essays and case studies, that “raw data” is an oxymoron, like “jumbo shrimp”. The authors each argue that data is always “cooked”, meaning that it always the result of human work, assumptions, and biases that will privilege certain information at the exclusion of others. The essays each take a historical perspective, ranging from such topics as the interpretation of ancient astronomical records to a case study about the data practices involved in long-term scientific field studies. The focus of the book centers more on the collection and processing of data, rather than on analysis thereof. 


---


***Data Before the Fact, by Daniel Rosenberg***

In this essay, Rosenberg (a professor of History at the University of Oregon) discusses etymological history of the term data, specifically by investigating its usage in 17th and 18th century Britain. As stated in the first paragraph, “The rise of the concept [of data] in the seventeenth and eighteenth centuries are tightly linked to the development of modern concepts of knowledge and argumentation” (pg 17)—the point of the chapter is to “sketch the early history of the concept of “data” in order to understand the way in which that space was formed”. Rosenberg situates “data” against other similar terms like “fact” and “evidence” and shows that the term “data” serves a more strictly rhetorical purpose. Data are seen to represent the world as it is, and even false or bad data remains data. A wrong or false fact, however, ceases to be a fact.

Rosenberg explores the history of “data” both through traditional means, but also more quantitatively, using google N-gram viewer and another tool called ECCO. Doing so, Rosenberg makes three key claims: (1) “Data” came into being between the 17th and 18th centuries; (2) it emerged principally through discussions of mathematics and theology; and (3) over the 18th century, the meaning of “data” shifted from a “principals accepted as a basis of argument or facts gleaned from scripture” to instead mean “facts and evidence determined by experiment, experience, or collection”. So the term “data” exists and was used pre-enlightenment, but its meaning shifted heavily post. 

“Data means, and has meant for a very long time—that which is given prior to argument. As a consequence, the meaning of data must always shift with argumentative strategy and context—and with the history of both. The rise of modern economics and empirical natural science created new conditions of argument and new assumptions about facts and evidence. And the histories of those terms and others in the same family nicely illustrate the larger epistemological developments” (pg 36)

----

***Procrustean Marxism and Subjective Rigor: Early Modern Arithmetic and Its Readers: Travis D Williams ***

This essay by Travis Williams (An English professor at the University of Rhode Island) is stated to demonstrate that “a dataset is always interpreted by the fact that it is a set: some elements are privileged by inclusion, while others are denied relevance through exclusion.” He conducts this demonstration by analyzing the history of mathematics. Williams claims that when studying old arithmetic books, the “economic function” of mathematics to the then-industrializing society was always highlighted, whilst other elements were removed or ignored, considered to be inconsequential. These removed elements included sometimes bizarre mathematical reasoning, problem descriptions involving drunkards and maidens with broken eggs. However, Williams argues that some of these elements are essential to understanding arithmetic itself, as well as its historical significance, and removing that limits our understanding of history. 

One of Williams’s key points is that math is not “just math”—we always assume that math is universal, and thus “our” math is the same as “their” match (where in Williams’ essay “their” math is that of early-modern Britain). However, Williams argues that math is not so universal, and that “the arithmetic of early modern Britain is not our arithmetic. Its preoccupations, variety and inscrutability render it distinct. Some of its practices today could only be described as errors and swept aside. But a responsible historiography must make its task to read that error as significant, even as potentially central to whatever early modern arithmetic was doing for its readers in its own culture” (pg 52) By assuming that the math of early modern Britain was our math, historians swept aside and ignored various aspects of its use. In pointing this out, Williams makes a double argument: even the most seemingly-objective and universal studios are in fact contextual, and that bringing “our” assumptions to the study of history (or other disciplines) causes us to privilege certain information at the expense of others, even when their relative value is not so obvious. 

“Our assumptions about how and with what accuracy early modern readers solved arithmetic problems fundamentally affects our ability to understand how and wh early modern readers read their own cultures’ arithmetic texts. To put is yet another way, the assumption that our rigor was their rigor allows our reading to ignore the procedures and accuracy of theri arithmetic because it can be assumed that they are the same: that their procedures are ours and that they always got the answers right, just as we assume we would do.” (pg 43)

----

***From Measuring Desire to Quantifying Expectations: A Late Nineteenth-Century Effort to Marry Economic Theory and Data, (Kevin R. Brine and Mary Poovey)***

Kevin Brine (an author of various finance books) and Mary Poovey (a cultural historian at NYU) examine the work of Irving Fisher, a famous economics who revolutionized his field, chiefly by changing how economists think about data and its relation to theory. What they show is that “not only is economic data never raw, then, in the sense of being interpreted, but also the form that makes data suitable for economists’ use carries with it assumptions about quantification and value that now fo unnoticed and unremarked” (pg 61) 

Honestly, I found this chapter to be quite boring and so did not read in detail. However, from what I could glean, it follows a 5-year period in which Fisher was developing his early theory of market equilibrium, the concept of “capital”, and more. A key issue in economics was that it was difficult to measure market forces and customer desires. In attempting to simplify and study these phenomenon, Fisher first created a physical device, involving cisterns of water, that could be used to conceptualize of “equilibrium”. While this model was rich in theory, it lacked empirical data. Later, Fisher would develop his thoughts in the book Appreciation and Interest in which is applied his ideas to data. “In marrying theory to empirical data in a quantitative form, Fisher created a framework that could fit the theory to the data—but only if various kinds of data were made commensurate with each other and only if the data embedded the assumptions that made them usable in the first place” (pg 72) This concept of “made commensurate” refers to the labor used to “scrub” or “clean” the data, making sure that it is in the correct form for use. 

The chapter closes by mentioning Fisher’s contributions to economics, listed as (1) that what the economist’s needed to quantify was not the force of desire but expectations about the future; and (2) because he recognized the usefulness of the present-value calculations by which real-world actors routinely presented these expectations, he helped naturalize an old set of assumptions about money and value. 

“...economist’s data always exists at several degrees of remove from the world of market transactions. The numbers that prove useful for the economist’s calculations, in other words, already embed a set of conventional assumptions, which are simultaneously reinforced and effaced in technologies like present-value calculations and compound interest tables. When the economist scrubs such data t make it more amenable to the calculations he wants to make, he repeats a process of elaboration and obfuscation that is already implicit in them.”


----

***Where is the Moon, Anyway? The Problem of INterpreting Historical Solar Eclipse Observations, Matthew Stanley***

In this paper, Matthew Stanley (a historian at NYU) shows how even data collection in the field of Astronomy, a discipline viewed as the peak of the hierarchy of sciences and within which strong consensus can be obtained, can actually still be quite contextual. Stanley focuses on the study of a phenomenon called “Secular Acceleration”—an aspects of the moon’s movement that is best studied using long-term time and geographic data on solar eclipses. However, “acquiring this simple information for eclipses in the past might seem straightforward, but is actually the result of complicated, messy, processes that are far from standardized”. Indeed, these processes require the consultation of ancient records written in ancient Greek, CHinese, Cuneiform, and more. These records are often subjective accounts, more poetic than objective, or else lack important contextual information such as time and place. Interpretation of accounts differed by astronomer, with some seeing poetic and emotional language as more reliable—the eclipse left an impression—whereas other astronomers saw this as overly poetic and subjective, and thus unreliable. Many of these documents even lacked the word “eclipse”, and so filtering was, and remains a difficult and unreliable process. 

This all shows that even in astronomy, data relies on interpretation, interpretation that differs by the viewer. Astronomers, even with their high degree of consensus, simply cannot agree on how to interpret this sort of data. In their conclusion, Stanley states that “Astronomers as a group are simply not trained in the skills necessary for understanding these sources. Without this common background, it is extremely hard to achieve consensus that underlies, say, the mass of an electron. Until Sumerian is taught alongside celestial mechanics in graduate programs, it is unlikely that the secular acceleration will become settled knowledge” (pg 85). 

“The goal of all these struggles was obtaining a number: the secular acceleration, which would the modify the equations of the moon’s motion. To get this number, one needed other numbersL the time and place of ancient eclipses. But this data only existed once it had been passed through textual, historical, and psychological filters. Each filter could be used positively or negatively, to either exclude a record from reliability or to detect reliable records”


----

***Facts and FACTS: Abolitionists’ Database Innovations***

This chapter, written by Ellen Garvey (an English professor at New Jersey City University), traces the movements of abolitionists in the decades before the American Civil War. These early civil rights activists pioneered an early kind of data mining, one that made use of seemingly innocuous pieces of information in order to paint a damning picture of slavery. Sarah and Angelina Grimké heard from a sister, an English abolitionist, that “compiling concrete sources≠fact and statistics—such as the high percentage of British sailors who perished on slave ships, gleaned by patiently combing through ship’s logs—was far more effective in turning public opinion than appeals to sentiment. Data will out.” (pg 90) Thus, the Grimké sisters turned towards using the very words of the slave owners against them. 

The sister began collecting large archives of newspapers published in the territories of the Southern United States, obtaining several years worth and containing many thousands of papers. Then, they would go through these papers and identify what were seemingly innocuous pieces of information (at the time, still pretty terrible), but which when strung together formed an especially damning image. A central focus was advertisements of Runaway slaves, which often featured descriptions of the slaves themselves. By identifying some of these features: broken teeth, deformed limbs, scars, brandings, etc., the Grimké were able to paint a map of abuse and suffering across the south. And rather than simple whistle-blowing, they did this without any subterfuge or by revealing some secret—simply through the power of search and patience, they made use of already-present information for an entirely new purpose. Or, as stated by Garvey: “These advocates thus took an undifferentiated pile of ads for runaway slaves, wherein dates and places were of primary importance, rendered in neutral language of commerce, and transformed them into data about the routine and accepted torture of enslaved people” (pg 93)

“The extraordinary repurposing, reuse, and, most importantly, reconceptualizing and new juxtapositioning of media represented by American Slavery As It Is entailed a complex negotiation between modes of access to media, expertise, and the imagination to see that Southern newspapers not only could be made to speak against themselves, but also could be picked through, tagged, and sorted to support a new mode of understanding. That new mode of understanding might be called informatic, though informations—like computers—of course lay many years in the future. Weld and the Grimké’s arrive at an informatics sensibility out of the growing sense of urgency that abolitionists felt—the sense that simply softening the hearts of slaveholders was ineffectual and that hard facts were needed—which impelled them to turn to a new way of working” (pg 99)

----

***Chapter 6: Paper as Passion***

Skipped, not relevant, not interesting. Apparently about how a method of information organization wherein scholars organize key pieces of information read from texts on note cards, pointing to how what is being read can be applied to future uses. In doing so, they reduce the information and make it useful.

----

***Chapter 7: Dataveillance and Countervailance***

Also mostly skipped. It's about the ways in which we are being tracked, and explores from a somewhat philosophical angle what sort of “self” is captured by this data and what consequences might be for society. The second half discusses “counter” methods, either individual or societal, to counter this “dataveillance” paradigm. I think that “Antisocial Media” or some other works on surveillance are much better and more applicable. 

----

***Data Bites Man: The Work of Sustaining a Long-Term Study***

Written by David Ribes (a professor at University of Washington) and Steven J Jackson (at the iSchool of Cornell), this chapter makes a very clearly stated point: “the work of producing, preserving, and sharing data reshapes the organization, technological, and cultural worlds around them”. To elaborate on this, the authors use the examples of corn and drosophila (fruit flies). Both of these organisms have been engineered, for different purposes, by humans: they can not entirely live on their own, and have been modified and cultivated to not entirely resemble their “wild” ancestors. We have shaped these organisms. However, these organisms have also shaped us. We have built infrastructures, occupations, policies, and knowledge around each of these organisms—corn provides certain chemicals which we have built factories and processes to extract and refine. Meanwhile, study of drosophila allowed new kinds of experts and new approaches to the generation of scientific knowledge. These organisms also shape us. The authors argue that data is like these organisms—they are objects which have both been shaped, and which continue to shape us. The authors show this by telling “stories of data production that reveal the complex assemblages of people, places, documents, and technologies that must be held in place to produce scientific data” (pg 147), particularly in the case of a long-term study involving the collection of samples from streams around Baltimore Country. 

Sustaining a long-term study is a lot of work, and has some tricky dilemmas. Data collected over time must be made commensurate—processes, tools, techniques, and measure used in one week should be maintained over the course of the study so that data can be compared and trends generalizable. However, issues arise: tools break, or are stolen; people build houses and factories nearby streams; streambeds dry up; equipment becomes outdated, and much more. These issues threaten to compromise any data analysis, and so must be taken care of. However this presents an epistemic challenge: the goal of the long-term study is to observe change over time; “transformations are expected, how do you know whether a change is revealing or compromising?” (pg 159)

The authors also discuss the various rituals and processes that take place in making supposedly “raw” data. These processes are fundamentally human, requiring the work of many people, technologies, objects, chemicals, organisms, and more. In doing so, they must fight to maintain commensurability, or in the author’s words: “These ecologists fight a three-front war with their closest allies. In order to sustain a comparison archive, data demand the taming of unruly field sites, humans, and infrastructures A dance of stability and change emerges in an ongoing effort to isolate environmental transformations that can stand in for something broader than a streambed”. These local struggles of scientists to develop knowledge are the same processes that sustain and build global knowledge, and keep the scientific system running.  

“A data center is just that, a center of a much larger and much more complex network that extends all the way from field sites and laboratories to desktop computers at universities in every corner of the world. Push beyond the chrome exterior of the data center and you will find a squeamish student taking spit samples and delivering them to a genomics lab; scratch the silicon surface and you’ll uncover a frustrated field technician re-calibrating a vandalized weather monitoring station for the third time that month, or a professor pleading with a country clerk for access to the latest tax assessment records. For, as we will demonstrate, data have domesticated science not only in the sanitized environments of the industrial data center, but also at every stage, moment, and site of scientific activity. In order to support our growing appetite for scientific knowledge, we have entered into a symbiotic relationship with remaking our material, technological, geographical, organizational, and social worlds into the kind of environments in which data can flourish” (pg 152)
 

